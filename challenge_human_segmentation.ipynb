{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJEOtfKJtYnH"
   },
   "source": [
    "# Challenge image semantic segmentation\n",
    "\n",
    "### Antoine Habis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rQj-0iwHyRq"
   },
   "source": [
    "### Requirements: Tensorflow==1.15.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yr1aaiZYqeQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models in c:\\users\\antoine\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: image-classifiers==1.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: efficientnet==1.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from segmentation_models) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from efficientnet==1.0.0->segmentation_models) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.18.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.4)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.1.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.6.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.14.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\antoine\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (45.2.0.post20200210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import segmentation_models as sm\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "# keras.callbacks.callbacks\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLuOoqDWqeQK"
   },
   "source": [
    "### Import images and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8u0RCw1qeQL"
   },
   "outputs": [],
   "source": [
    "K_color = np.load('DATAS/K_color_intrinsic.npy')\n",
    "K_depth = np.load('DATAS/K_depth_intrinsic.npy')\n",
    "K_color_depth = np.load('DATAS/T_color_to_depth_extrinsic.npy')\n",
    "\n",
    "color_image = cv2.imread('DATAS/color_image.jpg')\n",
    "depth_image_colored = cv2.imread('DATAS/depth_image_colored.jpg')\n",
    "depth_image_mask = cv2.imread('DATAS/depth_image_mask.png')\n",
    "bonus_image = cv2.imread('DATAS/color_image_bonus.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7AsPsSjqeQO"
   },
   "outputs": [],
   "source": [
    "K_color_depth = K_color_depth[0:3,0:3]\n",
    "nb_pixels = depth_image_colored.shape[0] * depth_image_colored.shape[1]\n",
    "print(nb_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBMC3Xp_qeQT"
   },
   "outputs": [],
   "source": [
    "print(depth_image_colored.shape)\n",
    "print(color_image.shape)\n",
    "print(depth_image_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byIoNJvSqeQW"
   },
   "source": [
    "### Observation of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlFxxsM8qeQX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,3) \n",
    "\n",
    "# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "axarr[0].imshow(depth_image_colored)\n",
    "axarr[1].imshow(color_image)\n",
    "axarr[2].imshow(depth_image_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXlcAd2FqeQa"
   },
   "source": [
    "#### Different type of encoding for the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnNYOiK7qeQb"
   },
   "outputs": [],
   "source": [
    "array_mask = depth_image_mask[:,:,0]/255\n",
    "array_depth = np.float32(depth_image_colored.reshape(1280*720,3))\n",
    "array_color = np.float32(color_image.reshape(1280*720,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXCm8g8rqeQe"
   },
   "source": [
    "### Correct the alignment of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVGF7g_1qeQf"
   },
   "outputs": [],
   "source": [
    "def get_coordinate(pixel_number):\n",
    "    return np.array([pixel_number % 720, pixel_number//720])\n",
    "\n",
    "\n",
    "def get_number_pixel(coord):\n",
    "    return coord[1]*720 + coord[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euCyQf1nqeQi"
   },
   "outputs": [],
   "source": [
    "W = np.arange(1280*720)\n",
    "P = get_coordinate(W) + 1\n",
    "Q = np.vstack((P, np.ones(1280*720)))\n",
    "V = np.round(np.dot(K_depth, np.dot(\n",
    "    K_color_depth, np.dot(np.linalg.inv(K_color), Q)))).astype(int)\n",
    "V = V[0:2]\n",
    "get_number_pixel(V)\n",
    "new_array = np.array([array_depth[u] for u in get_number_pixel(V)])\n",
    "new_img = new_array.reshape(1280, 720, 3).astype(np.uint8)\n",
    "plt.imshow((new_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FANRUoT5qeQm"
   },
   "source": [
    "### For the rest of the challenge we will use the new image as the depth colored image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGWGKtB9qeQn"
   },
   "outputs": [],
   "source": [
    "depth_image_colored = new_img\n",
    "array_depth = np.float32(depth_image_colored.reshape(1280*720,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsXdtAuAqeQq"
   },
   "source": [
    "The image of the mask can be represented as an array with size (1280, 720) (Black and White img)\n",
    "However the two other images are represented by arrays with size (1280,720,3) (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlHkT_IOqeQr"
   },
   "source": [
    "### Definition of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45_I-Ty5qeQs"
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, prt=True):\n",
    "    acc1 = 1 - np.sum(np.abs(labels - array_mask))/nb_pixels\n",
    "    labels = 1 - labels\n",
    "    acc2 = 1 - np.sum(np.abs(labels - array_mask))/nb_pixels\n",
    "    if prt:\n",
    "        print('the accuracy of the algorithm is {}'.format(max(acc1, acc2)))\n",
    "    return max(acc1, acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRFp4oZaqeQu"
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_PYc2hVqeQv"
   },
   "source": [
    "### Observation of k-means algorithm separately on both images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otGGC3mKqeQw"
   },
   "source": [
    "#### K- means on depth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEvYT5peqeQx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compactness_d, labels_d, centers_d = cv2.kmeans(array_depth, 2, bestLabels = None,criteria = (cv2.TERM_CRITERIA_MAX_ITER, 10000, 1e-12), attempts = 5, flags = cv2.KMEANS_PP_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKcrSh1eqeQz"
   },
   "outputs": [],
   "source": [
    "compactness_c, labels_c, centers_c = cv2.kmeans(array_color, 2, bestLabels = None,criteria = (cv2.TERM_CRITERIA_MAX_ITER, 10000, 1e-12), attempts = 5, flags = cv2.KMEANS_PP_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziX6RcYSqeQ3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_d = labels_d.reshape(1280,720)\n",
    "plt.imshow(labels_d,'gray')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOF8YMIcqeQ-"
   },
   "outputs": [],
   "source": [
    "accuracy(labels_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDl81-q8qeRA"
   },
   "source": [
    "#### K- means on colored image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTyffX15qeRB"
   },
   "outputs": [],
   "source": [
    "labels_c = labels_c.reshape(1280, 720)\n",
    "plt.imshow(labels_c, 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7pWLV2nqeRE"
   },
   "outputs": [],
   "source": [
    "accuracy(labels_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nckq14sLqeRH"
   },
   "source": [
    "### K-means with channel = 6 (the two images with 2 RGB tuples as features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XIET7c_qeRI"
   },
   "source": [
    "As We can see in the original images, the depth image seems to be more correlated to the target than the colored image is so let's add a compensation parameter $C = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVZZ-B_LqeRJ"
   },
   "outputs": [],
   "source": [
    "C = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_6lD5CaqeRM"
   },
   "outputs": [],
   "source": [
    "array_concat = np.hstack((array_depth,C*array_color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvYFHyDjqeRO"
   },
   "outputs": [],
   "source": [
    "compactness, labels, centers = cv2.kmeans(array_concat, 2, bestLabels = None, criteria = (cv2.TERM_CRITERIA_EPS, 10, 2000), attempts = 5, flags = cv2.KMEANS_PP_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWNvTdT_qeRR"
   },
   "outputs": [],
   "source": [
    "labels = labels.reshape(1280,720)\n",
    "plt.imshow(labels,'gray')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqewXxKaqeRT"
   },
   "outputs": [],
   "source": [
    "accuracy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rukxGSaqeRW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "for u in np.linspace(10, 5000):\n",
    "    v = cv2.kmeans(array_concat, 2, bestLabels=None, criteria=(\n",
    "        cv2.TERM_CRITERIA_EPS, 1, u), attempts=5, flags=cv2.KMEANS_PP_CENTERS)[1]\n",
    "    v = v.reshape(1280, 720)\n",
    "    acc_list.append(accuracy(v, False))\n",
    "plt.plot(np.linspace(10, 5000), acc_list)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('evolution of the accuracy as function of epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ey83dKEqeRY"
   },
   "source": [
    "The accuracy occilates between 80% and 90% but we cannot see a pattern in the evolution of the accuracy. The optimal value of $\\epsilon^*$ is too correlated with the initialization of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5XyLasrqeRZ"
   },
   "source": [
    "## The algorithm is not good enough, let's try to preprocess the image first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyStMb3_qeRa"
   },
   "source": [
    "###### Let's first try to blur the image to get rid of some misclassified isolated points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QATaNFdLqeRb"
   },
   "outputs": [],
   "source": [
    "kernel_size = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgUIiKmlqeRd"
   },
   "source": [
    "### Observation of the blured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v5oRDH5qeRe"
   },
   "outputs": [],
   "source": [
    "# Remove noise\n",
    "# Gaussian\n",
    "blur_depth = cv2.GaussianBlur(\n",
    "    depth_image_colored, (kernel_size, kernel_size), 0)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(depth_image_colored)\n",
    "axarr[1].imshow(blur_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho0nUNg-qeRh"
   },
   "outputs": [],
   "source": [
    "blur_colored = cv2.GaussianBlur(color_image, (kernel_size, kernel_size), 0)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(color_image)\n",
    "axarr[1].imshow(blur_colored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnVOKZPoqeRn"
   },
   "outputs": [],
   "source": [
    "blur_depth = np.float32(blur_depth.reshape(1280*720, 3))\n",
    "blur_colored = np.float32(blur_colored.reshape(1280*720, 3))\n",
    "array_concat_blured = np.hstack((blur_depth, C*blur_colored))\n",
    "t1 = time.time()\n",
    "compactness_blur, labels_blur, centers_blur = cv2.kmeans(array_concat_blured, 2, bestLabels=None, criteria=(\n",
    "    cv2.TERM_CRITERIA_EPS, 500, 2000), attempts=5, flags=cv2.KMEANS_PP_CENTERS)\n",
    "t2 = time.time()\n",
    "print('the algorithm took {} seconds to run'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vlpZYs1qeRp"
   },
   "source": [
    "### Accuracy with k-means with blured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6S6f0k61qeRq"
   },
   "outputs": [],
   "source": [
    "labels_blur = labels_blur.reshape(1280,720)\n",
    "plt.imshow(labels_blur,'gray')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-vgW-U6qeRs"
   },
   "outputs": [],
   "source": [
    "accuracy(labels_blur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S799dm2dqeRv"
   },
   "source": [
    "We are getting closer here with an accuracy of \n",
    "\n",
    "$acc \\approx 84\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yxms0Tn7qeRw"
   },
   "outputs": [],
   "source": [
    "img_blur_col = blur_colored.reshape(1280,720,3).astype('uint8')\n",
    "img_blur_depth = blur_depth.reshape(1280,720,3).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-g0oz6jqeRy"
   },
   "outputs": [],
   "source": [
    "matplotlib.image.imsave('blur_color.png', img_blur_col)\n",
    "matplotlib.image.imsave('blur_depth.png', img_blur_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MjourojqeR1"
   },
   "source": [
    "## cvtColor with adaptative threshold for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RRUfH82qeR3"
   },
   "source": [
    "###### Let's try this  algorithm with the blured images we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHYxm5SPqeR4"
   },
   "outputs": [],
   "source": [
    "cv2.cvtColor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3x5o78s0qeR6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_d = cv2.imread('blur_depth.png')\n",
    "gray_d = cv2.cvtColor(img_d,cv2.COLOR_BGR2GRAY)\n",
    "ret_d, thresh_d = cv2.threshold(gray_d,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "img_output_d = np.uint8(thresh_d)/255\n",
    "plt.imshow(img_output_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbHovd5fFDUu"
   },
   "outputs": [],
   "source": [
    "accuracy(img_output_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXFQEKo9qeR8"
   },
   "outputs": [],
   "source": [
    "img_c = cv2.imread('blur_color.png')\n",
    "gray_c = cv2.cvtColor(img_d,cv2.COLOR_BGR2GRAY)\n",
    "ret_c, thresh_c = cv2.threshold(gray_c,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "img_output_c = np.uint8(thresh_c)/255\n",
    "plt.imshow(img_output_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb3yxHi7FG_2"
   },
   "outputs": [],
   "source": [
    "accuracy(img_output_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5g1tec4qeR-"
   },
   "source": [
    "##### Now let's try with a weighted sum of the two images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVprcMs_qeR_"
   },
   "outputs": [],
   "source": [
    "p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCkkNwUiqeSE"
   },
   "outputs": [],
   "source": [
    "mixt_img = img_blur_col*p + (1-p)*img_blur_depth\n",
    "mixt_img = mixt_img.astype('uint8')\n",
    "matplotlib.image.imsave('mixt.png', mixt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EBT3Aj3qeSG"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('mixt.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "output = np.uint8(thresh)/255\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXZVYIngFK3_"
   },
   "outputs": [],
   "source": [
    "accuracy(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y_QKduxqeSJ"
   },
   "source": [
    "The algorithm is not good enough, Kmeans is still the best option so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oA_WOjRqeSJ"
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "# IMPORTANT: If you don't want to load the data or train the model just execute the cell load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-gQue8_qeSs"
   },
   "outputs": [],
   "source": [
    "width = 512 #### dimension of the image to pass into the network \n",
    "height = 256\n",
    "channel = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywkN_ewqmhoe"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "#### The dataset is composed of images that are divided into two parts:\n",
    "\n",
    "- The true image on the left side\n",
    "- the image with human segmentation on the right side\n",
    "\n",
    "\n",
    "#### /!\\   The following cell takes a lot of time to execute \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uda85YrAqeSu"
   },
   "outputs": [],
   "source": [
    "#### import sys\n",
    "files = glob.glob(\"/content/drive/My Drive/DATAS/masks_human/*.png\")\n",
    "n = len(files)\n",
    "images= []\n",
    "for i, filename in enumerate(files):\n",
    "  img = cv2.imread(filename)\n",
    "  img = cv2.resize(img, (width,height))\n",
    "  images.append(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSbPZ1_uqeSy"
   },
   "outputs": [],
   "source": [
    "images = np.asarray(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Jq31bXEoLLp"
   },
   "source": [
    "### observation of an image in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJ_9U-aUoOn_"
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WeWkCBAqeS0"
   },
   "outputs": [],
   "source": [
    "# images = np.concatenate(images, axis=0)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vem4Ar5znPN5"
   },
   "source": [
    "### First preprocessing of the data to train the model afterwards\n",
    "\n",
    "X is the left side of the image.\n",
    "\n",
    "To get y we need to take the left part and the right part of the images and compare them.\n",
    "\n",
    "if any pixel value has changed then it means that this pixel belonged to the human body so we set it's value to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqKaKMEdqeS3"
   },
   "outputs": [],
   "source": [
    "X  = images[:,:,:width//2,:]\n",
    "X.shape\n",
    "y  = images[:,:,width//2:,:]\n",
    "y.shape\n",
    "\n",
    "y = np.abs(y - X)\n",
    "y = np.sum(y, axis = 3)\n",
    "y = y>0\n",
    "y = np.expand_dims(y, axis = 3)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pd6xVGUmqeS5"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBtLFJPVQz26"
   },
   "outputs": [],
   "source": [
    "val1 = np.sum(y)\n",
    "tot = y.shape[0]*y.shape[1]*y.shape[2]\n",
    "weight = 1 -val1/tot\n",
    "print('there is {}% of background in the images of the dataset'.format(round(weight*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4ePIbTT8rGt"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-2tWD9aOomK"
   },
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy( y_true, y_pred, weight=weight) :\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "    logloss = -(y_true * K.log(y_pred) * weight + (1-weight)*(1 - y_true) * K.log(1 - y_pred))\n",
    "    return K.mean(logloss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfwBvDsyxIrz"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pae6jaAMqeS8"
   },
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "# preprocess input\n",
    "X_train_process = preprocess_input(X_train)\n",
    "X_test_process = preprocess_input(X_test)\n",
    "\n",
    "# define model\n",
    "model = sm.Unet(BACKBONE, classes=1, activation='sigmoid')\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "    loss=weighted_binary_crossentropy,\n",
    "    metrics=[sm.metrics.iou_score],\n",
    ")\n",
    "\n",
    "# fit model\n",
    "# if you use data generator use model.fit_generator(...) instead of model.fit(...)\n",
    "# more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n",
    "model.fit(\n",
    "    x=X_train_process,\n",
    "    y=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=40,\n",
    "    callbacks=[ModelCheckpoint('/content/drive/My Drive/DATAS/unet.hdf5', monitor='val_loss',\n",
    "                               verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)],\n",
    "    validation_data=(X_test_process, y_test),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SktlHmW0P8yH"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/My Drive/DATAS/unet.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANJuV3GcCEO0"
   },
   "outputs": [],
   "source": [
    "img_test = cv2.resize(color_image, (width//2, height))\n",
    "img_test = np.float32(img_test)\n",
    "img_test.shape\n",
    "test = np.expand_dims(img_test, axis=0)\n",
    "test = preprocess_input(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7clne7lMSo7"
   },
   "outputs": [],
   "source": [
    "# img_test = cv2.resize(depth_image_colored, (width,height)).round()\n",
    "# plt.imshow(img_test)\n",
    "# depth_image_colored = np.float32(img_test)\n",
    "# depth_image_colored.shape\n",
    "# test = np.expand_dims(depth_image_colored, axis = 0)\n",
    "# test = preprocess_input(test)\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnooBh4srXOR"
   },
   "source": [
    "## Observation of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKkaw9qBy3XX"
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "y_predict = model.predict(test)[0][:,:,0].round()\n",
    "t2 = time.time()\n",
    "print('the algorithm took {} seconds to run'.format(t2-t1))\n",
    "y_predict = cv2.resize(y_predict, (720,1280))\n",
    "plt.imshow(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFkPdueOYP0H"
   },
   "outputs": [],
   "source": [
    "accuracy(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6An1ZR_6XLM"
   },
   "source": [
    "# Commentraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2_0D3ZAdd_u"
   },
   "source": [
    "## For the unsupervised learning algorithm:\n",
    "\n",
    "The algorithm has no clue what are the two classes he made with it's classification technic. We saw that the K-means worked pretty well with the depth image because the person is in the foreground so the pixels are more likely to be red and yellow. However if any object (such as the chair on the right side) are  in the foreground too, then the algorithm will classify it as belonging to the human part.\n",
    "\n",
    "\n",
    "We can easily see that K-means will not work well on the color image but when we add the information of the color image to the depth image it's a bit better.\n",
    "\n",
    "With Kmeans preprocessed (blured) using both images combined we get an accuracy of 84% which is not bad but still not incredible because the classes are unbalaced (returning a black image gives more than 70% acc). \n",
    "However, the algorithm is very fast and take 0.25 seconds to run.\n",
    "\n",
    "\n",
    "\n",
    "The accuracy is calculated with the inverse of the MAE:\n",
    "$acc = 1 - \\frac{\\sum_i |\\hat{y_i} - y_i|}{n}$\n",
    "\n",
    "\n",
    "This accuracy is the most intuitive way to evaluate the model's prediction.\n",
    "We could have chosen a weighted accuracy but it's that important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BU9Ab5pMwxI"
   },
   "source": [
    "##  For supervised learning algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iM44q5rtRpWw"
   },
   "source": [
    "- The dataset on which we train is not big enough but it's an overview of what we can do with more data.\n",
    "\n",
    "\n",
    "First thing to notice is that the dataset is really unbalanced. \n",
    "In the picture there are way more pixel that belong to the class background than pixels that belong to class human.\n",
    "\n",
    "The idea is to change binary cross entropy and adjust it with weights that correspond to the proportion of the different class in the dataset.\n",
    "\n",
    "We get a loss that is defined like this:\n",
    "\n",
    "$loss = \\frac{\\sum_i -(w y_i log(\\hat{y}_i) + (1-w)(1 - y_i)log(1 - \\hat{y}_i))}{n}$\n",
    "\n",
    "I chose to train with the **resnet 34** which is the best BACKBONE from all the different BACKBONE I tried (vgg6, resnet50, inceptionv3,etc,...)\n",
    "\n",
    "The model I chose to predict the mask of the colored image is the best model according to the val_loss criteria during the backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "**To improve our segmentation:**\n",
    " As we've seen in this section, we are only using the colored image and not the depth image because we don't have a dataset with depth picture. What we could do is a probabilistic classification (for example GMM) on the depth image and combine the two predictions (with GMM with depth image and with deep learning and colored image) to get a better prediction.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7mKJVWmTZky"
   },
   "source": [
    "## Going Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Z9Bjzr7Xoyw"
   },
   "source": [
    "- **With unsupervised learning** it is impossible to get a good segmentation of the body because of the bottom part (which is cut) in the bonus image.\n",
    "\n",
    "- **With supervised learning** it is going to be hard because the dataset is labeled such that if there is a part of the body hidden on the picture then the hidden part will not be labeled as a part of the human body.\n",
    "\n",
    " 1. A solution would be to train on a dataset where the segmentation is done such as if a part of the body is hidden on the picture then the  data still recognize the hidden part as belonging to the human.\n",
    "\n",
    " 2. If we don't have such a dataset we can try something else and cut the images in the dataset ourselves. The idea would be to train a first convolutional network to reproduce the original image from the image that was cut and then apply another network for segmentation (for example the one implemented before).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRJ-r1xsMeAU"
   },
   "source": [
    "## Result of supervised Learning on the bonus image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y-x-82qruMo"
   },
   "outputs": [],
   "source": [
    "img_test = cv2.resize(bonus_image, (width//2,height))\n",
    "img_test = np.float32(img_test)\n",
    "img_test.shape\n",
    "test = np.expand_dims(img_test, axis = 0)\n",
    "test = preprocess_input(test)\n",
    "test.shape\n",
    "\n",
    "t1 = time.time()\n",
    "y_predict = model.predict(test)[0][:,:,0].round()\n",
    "t2 = time.time()\n",
    "print('the algorithm took {} seconds to run'.format(t2-t1))\n",
    "y_predict = cv2.resize(y_predict, (720,1280))\n",
    "plt.imshow(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVCstM8bMj5J"
   },
   "outputs": [],
   "source": [
    "accuracy(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref I looked at:\n",
    "\n",
    "- https://github.com/qubvel/segmentation_models\n",
    "- https://www.kaggle.com/c/person-segmentation-coco\n",
    "- https://github.com/matterport/Mask_RCNN\n",
    "- https://www.tensorflow.org/tutorials/images/segmentation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "challenge_Antoine_HABIS.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
